{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# _2nd code file of TAA project_\n",
    "\n",
    "This file includes:\n",
    "* code of all the models experimented with\n",
    "* the hyper-parameter tuning procedure\n",
    "* code to produce charts of their architecture\n",
    "* code to generate confusion matrix and table containing other metrics\n",
    "* code to do fine tuning with the transfer learning architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import InceptionV3\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Input, UpSampling2D, Flatten, BatchNormalization, Dense, Dropout, GlobalAveragePooling2D\n",
    "from keras.optimizers import Adam\n",
    "import pickle\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tensorflow.keras.applications.inception_v3 import preprocess_input\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading files\n",
    "To test with other files (for example, with the background removed) this file names should be changed to the appropriate ones and everything works fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pickle.load(open(\"X299.pickle\",\"rb\"))\n",
    "X = np.array(X)\n",
    "\n",
    "y = pickle.load(open(\"y299.pickle\",\"rb\"))\n",
    "y = np.array(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into training and testing data\n",
    "This split is stratified, which means that the ratios between the numbers of images in each class will be kept equal in the testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function that returns the (complex) model according to some variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getModel(dropout=.25, learning_rate=0.001, augmentation=False):\n",
    "  base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(299, 299, 3))\n",
    "  base_model.trainable = False\n",
    "\n",
    "  inputs = tf.keras.Input(shape=(299, 299, 3))\n",
    "  if augmentation:\n",
    "    x = tf.keras.Sequential([\n",
    "      tf.keras.layers.experimental.preprocessing.RandomFlip('horizontal'),\n",
    "      tf.keras.layers.experimental.preprocessing.RandomRotation(0.2),\n",
    "    ])(inputs)\n",
    "  else:\n",
    "      x = inputs\n",
    "  x = tf.keras.applications.inception_v3.preprocess_input(x)\n",
    "  x = base_model(x, training=False)\n",
    "  x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "  x = tf.keras.layers.Dense(256,activation='relu')(x)\n",
    "  x = tf.keras.layers.Dropout(dropout)(x)\n",
    "  x = tf.keras.layers.BatchNormalization()(x)\n",
    "  outputs = tf.keras.layers.Dense(37,activation='softmax')(x)\n",
    "  model = tf.keras.Model(inputs, outputs)\n",
    "  model.summary()\n",
    "\n",
    "  model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=learning_rate), metrics=['accuracy'])\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function that returns the simpler model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSimplerModel(neurons=256, dropout=.25, learning_rate=0.001):\n",
    "\n",
    "  inputs = tf.keras.Input(shape=(299, 299, 3))\n",
    "  x = tf.keras.applications.inception_v3.preprocess_input(inputs)\n",
    "  x = tf.keras.layers.Flatten()(x)\n",
    "  x = tf.keras.layers.Dense(neurons, activation='relu')(x)\n",
    "  x = tf.keras.layers.Dropout(dropout)(x)\n",
    "  x = tf.keras.layers.BatchNormalization()(x)\n",
    "  outputs = tf.keras.layers.Dense(37,activation='softmax')(x)\n",
    "  model = tf.keras.Model(inputs, outputs)\n",
    "  model.summary()\n",
    "\n",
    "  model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=learning_rate), metrics=['accuracy'])\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to encode `y` to arrays of 0's and 1's so it checks out with the model we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "def onehotencode_func(y):\n",
    "  integer_encoded = y.reshape(len(y), 1)\n",
    "  onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "  return onehot_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining values that will be experimented with and setting up k-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate_list = [0.01, 0.001]\n",
    "dropout_values_list = [0.25, 0.35]\n",
    "\n",
    "# 3-fold cross validation will be used because its computationally easier/faster\n",
    "kfold = StratifiedKFold(n_splits=3, shuffle=True)\n",
    "\n",
    "# dictionary where data will be stored\n",
    "hist = {'learning_rate': {}, 'neurons': {}, 'dropout': {}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions used later to save data from hyper parameter tuning on files, and to read them as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_in_file(parameter, dict, filename):\n",
    "  f = open(filename, 'ab')\n",
    "  pickle.dump({parameter : dict[parameter]}, f)\n",
    "  f.close()\n",
    "\n",
    "# returns a list\n",
    "def read_file(filename):\n",
    "  objs = [] \n",
    "  f = open(filename, 'rb')\n",
    "  while 1:\n",
    "      try:\n",
    "          objs.append(pickle.load(f))\n",
    "      except EOFError:\n",
    "          break\n",
    "  f.close()\n",
    "  return objs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function where information from k-fold cross validation will be averaged, stored and returned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the 'model_func' parameter is a lambda function\n",
    "def test_params(lr, model_func):\n",
    "  dic = {}\n",
    "  i = 0.0\n",
    "\n",
    "  # splitting data into the folds\n",
    "  folds = kfold.split(x_train, y_train)\n",
    "  for train_index, val_index in folds:\n",
    "\n",
    "    # getting the model with the desired parameters\n",
    "    model = model_func(lr)\n",
    "\n",
    "    x_train_kf, x_val_kf =  x_train[train_index], x_train[val_index]\n",
    "    y_train_kf, y_val_kf = onehotencode_func(y_train[train_index]), onehotencode_func(y_train[val_index])\n",
    "\n",
    "    # training the model with data from the train data folds\n",
    "    historytemp = model.fit(x_train_kf, y_train_kf, batch_size=32, epochs=15, validation_data=(x_val_kf, y_val_kf))\n",
    "\n",
    "    del model\n",
    "\n",
    "    if dic == {}:\n",
    "      # if dictionary is empty, values will be put there\n",
    "      dic['train_acc'] = np.array(historytemp.history['accuracy'])\n",
    "      dic['train_loss'] = np.array(historytemp.history['loss'])\n",
    "      dic['val_acc'] = np.array(historytemp.history['val_accuracy'])\n",
    "      dic['val_loss'] = np.array(historytemp.history['val_loss'])\n",
    "    else:\n",
    "      # if dictionary is not empty, values will be added element wise\n",
    "      dic['train_acc'] += np.array(historytemp.history['accuracy'])\n",
    "      dic['train_loss'] += np.array(historytemp.history['loss'])\n",
    "      dic['val_acc'] += np.array(historytemp.history['val_accuracy'])\n",
    "      dic['val_loss'] += np.array(historytemp.history['val_loss'])\n",
    "    \n",
    "    i+=1\n",
    "\n",
    "  for k in dic:\n",
    "    # each number in each array in the dictionary will be divided by the number of iterations, producing the mean of all the values read\n",
    "    dic[k] /= i\n",
    "\n",
    "  return dic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the experiences\n",
    "To use the model with the data augmentation layers, a parameter `augmentation=True` should be passed to the `getModel` functions\n",
    "\n",
    "To use the simpler model, the function that the lambda functions are calling should be changed to `getSimplerModel`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing learning rate:\n",
    "lr_model_func = lambda x : getModel(learning_rate=x)\n",
    "for lr in learning_rate_list:\n",
    "  hist['learning_rate'][lr] = test_params(lr, lr_model_func)\n",
    "\n",
    "save_in_file('learning_rate', hist, 'data299.pickle')\n",
    "\n",
    "\n",
    "# chaning dropout value:\n",
    "drop_model_func = lambda x : getModel(dropout=x)\n",
    "for d in dropout_values_list:\n",
    "  hist['dropout'][d] = test_params(d, drop_model_func)\n",
    "\n",
    "save_in_file('dropout', hist, 'data299.pickle')\n",
    "\n",
    "\n",
    "print(read_file('data299.pickle'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Displaying a representation of the neural network architecture\n",
    "Just like before, adding the parameter `augmentation=True` to the `getModel` function will add the data augmentation layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we get a model\n",
    "m = getModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# then we generate the diagram\n",
    "diagram_file = 'model_diagram_complex.png'\n",
    "im = tf.keras.utils.plot_model(\n",
    "    m, to_file=diagram_file, show_shapes=False, show_dtype=False,\n",
    "    show_layer_names=True, rankdir='TB', expand_nested=False, dpi=96\n",
    ")\n",
    "\n",
    "# this displays the diagram without title\n",
    "# display(im)\n",
    "\n",
    "# to add a title:\n",
    "img = [cv2.imread(diagram_file)]\n",
    "\n",
    "fig = plt.figure(figsize=(15,15))\n",
    "grid = ImageGrid(\n",
    "    fig,\n",
    "    111,\n",
    "    nrows_ncols=(1,1),\n",
    "    axes_pad=0.7\n",
    ")\n",
    "\n",
    "for ax, im in zip(grid, img):\n",
    "    ax.imshow(im)\n",
    "    ax.axis('off')\n",
    "\n",
    "fig.subplots_adjust(top=.95)\n",
    "fig.suptitle('Complex Neural Network Architecture Diagram', size='xx-large')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Displaying the confusion matrix and metrics table\n",
    "This was done after analysing the data (this is done on the next notebook) to find out the best parameters.\n",
    "\n",
    "Through our analysis, we discovered that the best parameters were:\n",
    "* **learning rate** = 0.001\n",
    "* **dropout value** = 0.35"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we get a new model and train it with all the training data available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = getModel(learning_rate=0.001, dropout=0.35)\n",
    "model.fit(x_train, onehotencode_func(y_train), batch_size=32, epochs=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the predictions of the test data and transforming it to a number (selecting the index of the maximum value and summing one):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_test)\n",
    "y_pred2 = [ np.argmax(i)+1 for i in y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of labels in order gotten from the previous notebook\n",
    "labels = ['abyssinian', 'american_bulldog', 'american_pit_bull_terrier', 'basset_hound', 'beagle', 'bengal', 'birman', 'bombay', 'boxer', 'british_shorthair', 'chihuahua', 'egyptian_mau', 'english_cocker_spaniel', 'english_setter', 'german_shorthaired', 'great_pyrenees', 'havanese', 'japanese_chin', 'keeshond', 'leonberger', 'maine_coon', 'miniature_pinscher', 'newfoundland', 'persian', 'pomeranian', 'pug', 'ragdoll', 'russian_blue', 'saint_bernard', 'samoyed', 'scottish_terrier', 'shiba_inu', 'siamese', 'sphynx', 'staffordshire_bull_terrier', 'wheaten_terrier', 'yorkshire_terrier']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function creates the confusion matrix\n",
    "# the first argument is the expected results, and the second is the obtained previsions\n",
    "cm = metrics.confusion_matrix(y_test, y_pred2)\n",
    "\n",
    "fig, ax = plt.subplots( figsize= (10, 10))\n",
    "\n",
    "ax.imshow(cm)\n",
    "\n",
    "ax.set_xticks(range(0, 37))\n",
    "ax.set_yticks(range(0,37))\n",
    "\n",
    "ax.set_yticklabels(labels)\n",
    "ax.set_xticklabels(labels, rotation='vertical')\n",
    "\n",
    "plt.xlabel('Predicted Class')\n",
    "plt.ylabel('True Class')\n",
    "plt.title('Confusion Matrix', size='xx-large')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this next function produces a table with values of precision, recall, f1-score and accuracy\n",
    "metrics.classification_report(y_test, y_pred2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning transfer learning model\n",
    "Having a model already trained on our data, we are going to unfreeze the previously frozen layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in model.layers:\n",
    "    if l.name == 'inception_v3':\n",
    "        base_model = l\n",
    "        break\n",
    "\n",
    "# unfreeze\n",
    "base_model.trainable = True\n",
    "\n",
    "# check how many layers in the base model\n",
    "print(\"Number of layers in the base model: \", len(base_model.layers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we are going to freeze all the layers before one specific chosen layer (rougly at 2/3 of the layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tune from this layer onwards\n",
    "fine_tune_at = 208\n",
    "\n",
    "# Freeze all the layers before the fine_tune_at layer\n",
    "for layer in base_model.layers[:fine_tune_at]:\n",
    "  layer.trainable =  False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learning rate used in this process should be lower because the model to be trained is huge, so smaller steps are better in order for it not to overfit\n",
    "Now we fit on the data once again for 10 more epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "              optimizer = tf.keras.optimizers.RMSprop(lr=0.001/10),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "fine_tune_epochs = 10\n",
    "total_epochs =  15 + fine_tune_epochs\n",
    "model.fit(x_train, y_train_2, batch_size=32,initial_epoch=initial_epochs, epochs=total_epochs, validation_split=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "metadata": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}